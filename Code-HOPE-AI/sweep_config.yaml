# sweep_config.yaml
#
# W&B Bayesian sweep over HPTransformerClassifier hyperparameters.
# Primary metric: val/f1 (maximise).
#
# Usage:
#   1. Create the sweep (run once):
#        wandb sweep sweep_config.yaml
#      → prints:  wandb: Created sweep <entity>/<project>/<SWEEP_ID>
#
#   2. Launch one or more agents (each agent runs `count` trials):
#        wandb agent <entity>/<project>/<SWEEP_ID> --count 30
#
#   The agent calls:
#        python train_transformer.py --hidden_dim <val> --lr <val> ...
#   using the CLI args defined in `parameters` below.
#   Path arguments (--train_csv, --kf_dir, etc.) fall back to the
#   argparse defaults inside the script — override them via `command` if needed.

program: train_transformer.py

command:
  - env
  - CUDA_VISIBLE_DEVICES=3
  - python
  - ${program}
  - ${args}

method: bayes        # bayes | grid | random

metric:
  name: val/f1
  goal: maximize

early_terminate:
  type: hyperband
  min_iter: 10       # don't kill runs before epoch 10

parameters:

  # ── Model ──────────────────────────────────────────────────────────────────
  hidden_dim:
    value: 512

  # n_heads must divide hidden_dim; all values (2, 4, 8) divide 128/256/512
  n_heads:
    values: [2, 4, 8]

  n_layers:
    values: [1, 2, 3, 4]

  dropout:
    distribution: uniform
    min: 0.0
    max: 0.5

  # ── Training ───────────────────────────────────────────────────────────────
  lr:
    distribution: log_uniform_values
    min: 1.0e-5
    max: 1.0e-3

  wd:
    distribution: log_uniform_values
    min: 1.0e-6
    max: 1.0e-2

  batch_size:
    value: 32       # fixed — not swept

  epochs:
    value: 25        # fixed — not swept

  