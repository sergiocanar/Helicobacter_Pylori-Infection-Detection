program: finetune_encoder.py
method: bayes
run_cap: 30

command:
  - env
  - CUDA_VISIBLE_DEVICES=1
  - python
  - ${program}
  - --fold
  - "2"          # ← change to 2 or 3 before registering the next sweep
  - ${args}

metric:
  goal: maximize
  name: val/f1

parameters:
  # Primary comparison: ImageNet init vs HOPE-AI (LSTMModel) init
  checkpoint:
    values:
      - weights/model.pth                # HOPE-AI / LSTMModel pretrained
      - weights/pvt_v2_b2.pth          # ImageNet pretrained (PVTv2-B2)


  # Learning rate — log-uniform range (bayes samples in log space)
  lr:
    distribution: log_uniform_values
    min: 1e-5
    max: 1e-3

  # Weight decay — log-uniform range
  weight_decay:
    distribution: log_uniform_values
    min: 1e-5
    max: 1e-2

  # Fixed training settings
  epochs:
    value: 30
  batch_size:
    value: 32
  img_size:
    value: 352
  warmup_epochs:
    value: 3
  num_classes:
    value: 2
  output_dir:
    value: outputs/sweep
